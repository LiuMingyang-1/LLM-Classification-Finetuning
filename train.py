import os
import pandas as pd
import torch
import numpy as np
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding
)
from peft import get_peft_model, LoraConfig, TaskType
from datasets import Dataset, load_from_disk
import shutil
import os
from datasets import load_from_disk, Dataset

# --- é…ç½® ---
# å»ºè®®å…ˆç”¨ 1.5B å¿«é€Ÿè·‘é€šæµç¨‹ï¼Œç¡®è®¤æ— è¯¯å†æ¢ 7B
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"  
MAX_LENGTH = 2048
OUTPUT_DIR = "./qwen_kaggle_output"
CACHE_DIR = "./qwen_data_cache"



def prepare_dataset(csv_path, tokenizer, cache_dir="./processed_data_cache"):
    # --- 1. å°è¯•è¯»å–ç¼“å­˜ ---
    if os.path.exists(cache_dir):
        print(f"âœ¨ å‘ç°ç¼“å­˜ç›®å½• '{cache_dir}'ï¼Œæ­£åœ¨ç›´æ¥åŠ è½½...")
        try:
            dataset = load_from_disk(cache_dir)
            print(f"âœ… åŠ è½½æˆåŠŸï¼åŒ…å« {len(dataset['train'])} æ¡è®­ç»ƒæ•°æ®ã€‚")
            return dataset
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ•°æ®æŸåï¼‰ï¼Œå°†é‡æ–°å¤„ç†ã€‚é”™è¯¯ä¿¡æ¯: {e}")

    # --- 2. å¦‚æœæ²¡ç¼“å­˜ï¼Œå¼€å§‹å¤„ç†æ•°æ® ---
    print("âš¡ æœªå‘ç°å¯ç”¨ç¼“å­˜ï¼Œå¼€å§‹ä»å¤´å¤„ç†æ•°æ®...")
    df = pd.read_csv(csv_path)
    df.fillna("", inplace=True)

    # æ ‡ç­¾æ˜ å°„
    def get_label(row):
        if row['winner_model_a'] == 1: return 0
        if row['winner_model_b'] == 1: return 1
        return 2 
    
    df['labels'] = df.apply(get_label, axis=1)
    
    # æ„å»º Prompt
    def construct_prompt(row):
        return (
            f"User Question: {row['prompt']}\n\n"
            f"Response A: {row['response_a']}\n\n"
            f"Response B: {row['response_b']}\n\n"
            f"Which response is better? Answer (Response A / Response B / Tie)."
        )
    
    df['text'] = df.apply(construct_prompt, axis=1)
    
    raw_dataset = Dataset.from_pandas(df[['text', 'labels']])
    
    def preprocess_function(examples):
        return tokenizer(
            examples['text'], 
            truncation=True, 
            max_length=MAX_LENGTH,
            padding=False 
        )
    
    print("ğŸš€ æ­£åœ¨ Tokenize (å¤šè¿›ç¨‹åŠ é€Ÿä¸­)...")
    tokenized_dataset = raw_dataset.map(
        preprocess_function, 
        batched=True,
        remove_columns=["text"], # å¿…é¡»åˆ é™¤ text åˆ—
        num_proc=4               # ğŸ”¥ã€æ–°å¢ã€‘å¼€å¯4ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œé€Ÿåº¦å¿«4å€
    )
    
    # åˆ’åˆ†æ•°æ®é›†
    split_dataset = tokenized_dataset.train_test_split(test_size=0.1)
    
    # --- 3. ä¿å­˜ç¼“å­˜ ---
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ° '{cache_dir}'ï¼Œä¸‹æ¬¡è¿è¡Œå°†æ— éœ€ç­‰å¾…...")
    split_dataset.save_to_disk(cache_dir)
    print("âœ… ä¿å­˜å®Œæˆï¼")
    
    return split_dataset

# --- 2. è¯„ä¼° ---
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

def main():
    # --- åŠ è½½ Tokenizer ---
    # Qwen ä¸éœ€è¦ trust_remote_code=True (ä½†åœ¨æŸäº›æ—§ç¯å¢ƒä¸­åŠ ä¸Šä¹Ÿä¸æŠ¥é”™)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # Qwen é»˜è®¤å°±æœ‰ pad_token (<|endoftext|> æˆ– <|im_end|>)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # --- å‡†å¤‡æ•°æ® ---
    dataset = prepare_dataset("train.csv", tokenizer)
    
    # --- åŠ è½½æ¨¡å‹ ---
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=3, 
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # Qwen å®Œç¾å…¼å®¹ï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½® config.pad_token_idï¼Œä½†ä¸ºäº†ä¿é™©ï¼š
    model.config.pad_token_id = tokenizer.pad_token_id
    
    # --- LoRA ---
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        inference_mode=False,
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],    
        
        )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # =========== âœ¨ æ ¸å¿ƒä¿®å¤ä»£ç å¼€å§‹ âœ¨ ===========
    # å¼ºåˆ¶å°†æ‰€æœ‰"å¯è®­ç»ƒå‚æ•°"è½¬ä¸º float32
    # è¿™ä¸€æ­¥èƒ½è§£å†³ "Attempting to unscale FP16 gradients" æŠ¥é”™
    # åŒæ—¶ä¹Ÿæé«˜äº† LoRA è®­ç»ƒçš„æ•°å€¼ç¨³å®šæ€§
    for name, param in model.named_parameters():
        if param.requires_grad:
            param.data = param.data.to(torch.float32)
    # =========== âœ¨ æ ¸å¿ƒä¿®å¤ä»£ç ç»“æŸ âœ¨ ===========

        # 4. Trainer å‚æ•°å¾®è°ƒ
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=1e-4, # LoRA å­¦ä¹ ç‡é€šå¸¸æ¯”å…¨é‡å¾®è°ƒå¤§ï¼Œä½† 2e-4 å¯èƒ½ç•¥å¤§ï¼Œ1e-4 æˆ– 5e-5 æ¯”è¾ƒç¨³
        per_device_train_batch_size=2, 
        gradient_accumulation_steps=8, # ç´¯è®¡æ­¥æ•°å¢åŠ ï¼Œç­‰æ•ˆ Batch Size = 2*8 = 16ï¼Œæ›´ç¨³å®š
        num_train_epochs=1,
        weight_decay=0.01,
        eval_strategy="steps",
        eval_steps=100,      # æ­¥æ•°ä¸ç”¨å¤ªé¢‘ç¹ï¼Œçœæ—¶é—´
        save_strategy="steps",
        save_steps=100,
        logging_steps=20,
        fp16=True,
        report_to="none",
        label_names=["labels"],
        warmup_ratio=0.03,   # å¢åŠ é¢„çƒ­ï¼Œé˜²æ­¢åˆšå¼€å§‹æ¢¯åº¦çˆ†ç‚¸
        metric_for_best_model="eval_loss", # ä»¥ loss ä¸ºå‡†
        greater_is_better=False,           # loss è¶Šå°è¶Šå¥½
        load_best_model_at_end=True        # è®­ç»ƒç»“æŸåŠ è½½æœ€å¥½çš„æ¨¡å‹
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=tokenizer,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
        compute_metrics=compute_metrics,
    )

    trainer.train()
    trainer.save_model(OUTPUT_DIR)

if __name__ == "__main__":
    main()